{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source: https://blog.csdn.net/u013719780/article/details/51743867\n",
    "# source: http://blog.csdn.net/u013719780?viewmode=contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# 1.分类特征变量的提取\n",
    "# 分类变量通常用独热编码（One-of-K or One-Hot Encoding），通过二进制数来表示每个解释变量的特征。\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "onehot_encoder = DictVectorizer()\n",
    "instances = [{'city': 'New York'},{'city': 'San Francisco'}, {'city': 'Chapel Hill'}]\n",
    "print(onehot_encoder.fit_transform(instances).toarray())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.文字特征提取\n",
    "# 很多机器学习问题涉及自然语言处理（NLP），必然要处理文字信息。文字必须转换成可以量化的特征向量。\n",
    "# 词库模型（Bag-of-words model）对于一个文档（document），忽略其词序和语法，句法，将其仅仅看做是一个词集合，\n",
    "# 或者说是词的一个组合，文档中每个词的出现都是独立的，不依赖于其他词是否出现\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "corpus = [\n",
    "    'UNC played Duke in basketball',\n",
    "    'Duke lost the basketball game']\n",
    "#文集包括8个词：UNC, played, Duke, in, basketball, lost, the, game。文件的单词构成词汇表（vocabulary）。\n",
    "#词库模型用文集的词汇表中每个单词的特征向量表示每个文档。我们的文集有8个单词，那么每个文档就是由一个包含8位元素的向量构成。\n",
    "#构成特征向量的元素数量称为维度（dimension）。用一个词典（dictionary）来表示词汇表与特征向量索引的对应关系。\n",
    "#在大多数词库模型中，特征向量的每一个元素是用二进制数表示单词是否在文档中。\n",
    "#例如，第一个文档的第一个词是UNC，词汇表的第一个单词是UNC，因此特征向量的第一个元素就是1。\n",
    "#词汇表的最后一个单词是game。第一个文档没有这个词，那么特征向量的最后一个元素就是0。\n",
    "#CountVectorizer类会把文档全部转换成小写，然后将文档词块化（tokenize）。\n",
    "#文档词块化是把句子分割成词块（token）或有意义的字母序列的过程。词块大多是单词，但是他们也可能是一些短语，如标点符号和词缀。\n",
    "#CountVectorizer类通过正则表达式用空格分割句子，然后抽取长度大于等于2的字母序列\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "print(vectorizer.fit_transform(corpus).todense())\n",
    "print(vectorizer.vocabulary_)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "————————————————\n",
    "版权声明：本文为CSDN博主「风雪夜归子」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。\n",
    "原文链接：https://blog.csdn.net/u013719780/article/details/51743867"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
